{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INSTALLATION NOTES\n",
    "LINUX\n",
    "\n",
    "`sudo apt install tesseract-ocr`\n",
    "\n",
    "MAC - Requires Homebrew\n",
    "\n",
    "`brew install tesseract`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not all these are necessary for the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! /bin/python3\n",
    "\n",
    "import os, os.path\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "import en_core_web_lg\n",
    "import json, string\n",
    "import datetime\n",
    "import warnings\n",
    "import re\n",
    "import sqlitedict\n",
    "from sqlitedict import SqliteDict\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nlp = en_core_web_lg.load()\n",
    "\n",
    "for word in nlp.Defaults.stop_words:\n",
    "    lex = nlp.vocab[word]\n",
    "    lex.is_stop = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *SPECIAL FUNCTION TO CHECK WORD PROPERTIES*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_words(doc):\n",
    "    '''\n",
    "    Helper function to check the properties of all words \n",
    "    in the passed portion of text. For data exploration.\n",
    "    '''\n",
    "    for sent in doc.sents:\n",
    "        for word in sent:\n",
    "            if word.like_url:\n",
    "                word.tag_, word.dep_ = 'URL', 'URL' # this\n",
    "#             if word.dep_ != 'punct' and word.pos_ != 'SPACE':\n",
    "            print('\"{}\"\\n LEMMA: {}\\n POS: {}\\n TAG: {}\\n DEP: {}\\n STOP: {}\\n'.format(word,\n",
    "                                                                                word.lemma_,\n",
    "                                                                                word.pos_, \n",
    "                                                                                word.tag_, \n",
    "                                                                                word.dep_,\n",
    "                                                                                word.is_stop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(doc):\n",
    "    '''\n",
    "    Returns a list of tuples including words of interest and their\n",
    "    count in the document. Using stop words to narrow search.\n",
    "    '''\n",
    "    counts = {}\n",
    "    restricted = {'punct', 'PUNCT', 'PART'}\n",
    "    for word in doc:\n",
    "        if word.is_stop is False and word.pos_ not in restricted:\n",
    "            if str(word) not in counts:\n",
    "                counts[str(word)] = 1\n",
    "            else:\n",
    "                counts[str(word)] += 1\n",
    "    return sorted([(v, k) for (k, v) in counts.items()], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_entities(doc):\n",
    "    '''\n",
    "    Returns a list of tuples including entities and their\n",
    "    count in the document.\n",
    "    '''\n",
    "    counts = {}\n",
    "    targets = {'NOUN', 'PROPN'}\n",
    "    restricted = {'punct', 'PUNCT', 'PART'}\n",
    "    for ent in doc.ents:\n",
    "        # all named entities are of interest\n",
    "        if str(ent) in counts:\n",
    "            counts[str(ent)] += 1\n",
    "        else:\n",
    "            counts[str(ent)] = 1\n",
    "    for word in doc:\n",
    "        if word.pos_ in targets and word.pos_ not in restricted:\n",
    "            if str(word) in counts:\n",
    "                counts[str(word)] += 1\n",
    "            else:\n",
    "                counts[str(word)] = 1\n",
    "    return sorted([(v, k) for (k, v) in counts.items()], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FUNCTIONS TO EXTRACT TEXT FROM SOURCE DOCUMENTS\n",
    "### Unnecessary since documents will be passed as string from front end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii(text):\n",
    "    text = ''.join([i if ord(i) < 128 else ' ' for i in text])\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+',' ', text)\n",
    "    text = re.sub(' . ', '', text)\n",
    "    return text\n",
    "\n",
    "def open_txt(path):\n",
    "    # Note the encoding here. Needed to remove BOM.\n",
    "    with open(path, 'r', encoding='utf-8-sig', errors='replace') as txt:\n",
    "        text = txt.read()\n",
    "        # spacy did not separate words with a \\n between\n",
    "        # strange punctuation character (â€˜) breaking spacy\n",
    "        text = remove_non_ascii(text)\n",
    "        text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_acronyms(doc):\n",
    "    acronyms = {}\n",
    "    chunks = set()\n",
    "    matches = {}\n",
    "    for word in doc:\n",
    "        if word.is_upper is True:\n",
    "            acronyms[str(word)] = word.text.lower()\n",
    "    for chunk in doc.noun_chunks:\n",
    "        text = chunk.text.lower()\n",
    "        text = re.sub('[^A-Za-z0-9 -]+', '', text)\n",
    "        words = text.split()\n",
    "        chunks.add((text, ''.join([word[0] for word in words])))\n",
    "    for i, j in acronyms.items():\n",
    "        for k, l in chunks:\n",
    "            if j in l:\n",
    "                matches[j.upper()] = k\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concepts(paragraph):\n",
    "    '''\n",
    "    SCHEMA CERTIFIED\n",
    "    Returns a list of dictionary objects containing an id number and the concept phrase.\n",
    "    '''\n",
    "    par = nlp(paragraph)\n",
    "    concepts = set()\n",
    "    \n",
    "    temp = set()\n",
    "    \n",
    "    watch = {'NOUN', 'PROPN'}\n",
    "    \n",
    "    for ent in par.ents:\n",
    "        temp.add(ent.text)\n",
    "    for noun in par:\n",
    "        # make sure noun is a noun\n",
    "        if not noun.like_url and not noun.like_email and noun.pos_ in watch and noun.pos_ != 'PRON':\n",
    "            temp.add(noun.text)\n",
    "    for chunk in par.noun_chunks:\n",
    "        for word in temp:\n",
    "            if word in chunk.text:\n",
    "                text = re.sub('[^A-Za-z0-9 -]+', '', chunk.text)\n",
    "                # manually modify to catch more mundane concepts\n",
    "                if len(text.split()) > 3:\n",
    "                    # ensure each concept is unique\n",
    "                    concepts.add(text.lower())\n",
    "                    \n",
    "    # catches and adds acronym full meanings based on local context\n",
    "    matches = match_acronyms(par)\n",
    "    for key, value in matches.items():\n",
    "        concepts.add(value)\n",
    "    # convert to list of dictionary objects per schema\n",
    "    \n",
    "    output = []\n",
    "    idx = 1\n",
    "    for con in concepts:\n",
    "        text = re.sub('a ', '', con)\n",
    "        text = re.sub('the ', '', text)\n",
    "        text = re.sub('an ', '', text)\n",
    "        text = re.sub('this ', '', text)\n",
    "        text = re.sub('that ', '', text)\n",
    "        # different from schema\n",
    "        # has to be done this way because keys must be unique\n",
    "        output.append({'id_{}'.format(idx): text})\n",
    "        idx += 1\n",
    "    concepts = output\n",
    "    \n",
    "    return concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence(paragraph):\n",
    "    sentences = []\n",
    "    par = nlp(paragraph)\n",
    "    idx = 1\n",
    "    for sent in par.sents:\n",
    "        # my prefered way\n",
    "        if len(sent.text) > 5: # crude filter to eliminate non-sentences\n",
    "            sentences.append({'id_{}'.format(idx): sent.text, 'vector': []})\n",
    "        idx += 1\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_paragraph_boundary(doc):\n",
    "    '''\n",
    "    Rules based paragraph delimiter. Helper function for SpaCy pipeline.\n",
    "    This will truncate paragraphs in which a hard new line has been used\n",
    "    in the middle of text.\n",
    "    '''\n",
    "    start = 0\n",
    "    seen_newline = False\n",
    "    # this is to make sure crazy white space separations are caught\n",
    "    targets = '\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n",
    "    for word in doc:\n",
    "        if seen_newline and not word.is_space:\n",
    "            yield doc[start:word.i]\n",
    "            start = word.i\n",
    "            seen_newline = False\n",
    "        elif word.text in targets:\n",
    "            seen_newline = True\n",
    "    if start < len(doc):\n",
    "        yield doc[start:len(doc)]\n",
    "\n",
    "def paragraphs(text):\n",
    "    '''\n",
    "    Resolves document down to paragraphs of substance just based on punctuation\n",
    "    and white space. No model so it is very fast.\n",
    "    '''\n",
    "    parags = English()\n",
    "    sbd = spacy.pipeline.SentenceSegmenter(parags.vocab, strategy=set_paragraph_boundary)\n",
    "    parags.add_pipe(sbd)\n",
    "    pars = parags(text)\n",
    "    paragraphs = [' '.join(sent.text.split()).strip() for sent in pars.sents]\n",
    "    \n",
    "    # this checks to see if paragraph is actually a paragraph\n",
    "    for par in paragraphs.copy():\n",
    "        if len(par.split('. ')) < 3: # this is only a check\n",
    "            paragraphs.remove(par) # this removes the string if it is not a paragraph\n",
    "    \n",
    "    output = []\n",
    "    for par in paragraphs:\n",
    "        par = nlp(par)\n",
    "        temp = []\n",
    "        for sent in par.sents:\n",
    "            if len(sent.text) > 5:\n",
    "                temp.append(sent.text)\n",
    "        output.append(' '.join(temp))\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knowledge(text):\n",
    "    '''\n",
    "    Assembly function to combine concepts and sentences using the\n",
    "    paragraph function.\n",
    "    '''\n",
    "    knowledge = []\n",
    "    idx = 1\n",
    "    for par in paragraphs(text):\n",
    "        knowledge.append({\n",
    "            # I do not recommend keeping both paragraph and sentence text\n",
    "            'id_{}'.format(idx): 'snippet',\n",
    "            'concepts': concepts(par),\n",
    "            'elements': {\n",
    "                'id': 'paragraph_{}'.format(idx),\n",
    "                'label': [chunk.text for chunk in nlp(par).noun_chunks][0].upper(),\n",
    "                'sentences': sentence(par)\n",
    "                \n",
    "            },\n",
    "            'weighted_queries': {},\n",
    "            'snippet_vector': []\n",
    "        })\n",
    "        idx += 1\n",
    "    return knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schema(text, path=''):\n",
    "    '''\n",
    "    Final schema assembly function.\n",
    "    '''\n",
    "    doc = nlp(text)\n",
    "    schema = {}\n",
    "    # this makes the assumption that the first chunk in the document will\n",
    "    # contain the title concept of the entire document.This may not be true.\n",
    "    schema.update({'id': [chunk.text for chunk in doc.noun_chunks][0].upper()})\n",
    "    schema.update({'url': path})\n",
    "    schema.update({'knowledge': knowledge(text)})\n",
    "    return schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Schema Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_directory(path):\n",
    "    db = SqliteDict('./data/knowledge.sqlite', autocommit=True)\n",
    "    textfiles = [path+f for f in os.listdir(path)]\n",
    "    for file in textfiles:\n",
    "        print(len(open_txt(file)), file)\n",
    "        db.update({file: schema(open_txt(file), file)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "176283 data/txt/1409.2544v1.pdf.txt\n",
      "20773 data/txt/0505016v1.pdf.txt\n",
      "26917 data/txt/1612.00712v1.pdf.txt\n",
      "40825 data/txt/0608073v1.pdf.txt\n",
      "29413 data/txt/0504056v1.pdf.txt\n",
      "31183 data/txt/1404.5997v2.pdf.txt\n",
      "32173 data/txt/1605.07333v1.pdf.txt\n",
      "16170 data/txt/1009.4495v1.pdf.txt\n",
      "49503 data/txt/1701.05549v1.pdf.txt\n"
     ]
    }
   ],
   "source": [
    "encode_directory('data/txt/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library has 9 items.\n",
      "\n",
      "{'id': 'THEEURALING', 'url': 'data/txt/1409.2544v1.pdf.txt', 'knowledge': [{'id_1': 'snippet', 'concepts': [], 'elements': {'id': 'paragraph_1', 'label': 'EXISTENCEFHISELATIONNDICATESHATUPPHILEHEREOESXISTNUCHHATHISONTRADICTSHESSUMPTIONHATUPPSIMPLICIALOMPLEX.EONCLUDEHATCASOYPEELATIONS', 'sentences': [{'id_1': 'existencefhiselationndicateshatupphilehereoesxistnuchhathisontradictshessumptionhatuppsimplicialomplex.eoncludehatCasoypeelations.', 'vector': []}, {'id_2': '40heanonicalormfChusnablessommediatelyeadff,iaheypeelations,heinimalorbiddenacesfheimplicialomplexCssociatedoheode,ndlsoheinimaleviationsfromeingimplicialomplex,hichreapturedyheypendypeelations.', 'vector': []}]}, 'weighted_queries': {}, 'snippet_vector': []}, {'id_2': 'snippet', 'concepts': [], 'elements': {'id': 'paragraph_2', 'label': \"[1].'KEEFEND.OSTROVSKY.HEIPPOCAMPUSSPATIALAP.RELIMINARYVIDENCEROMNITCTIVITYNHEREELY-MOVINGAT.RAINESEARCH,4(1):171175,971.2]..CNAUGHTON,..ATTAGLIA,.ENSEN,..OSER\", 'sentences': [{'id_1': \"[1].'Keefend.ostrovsky.heippocampusspatialap.reliminaryvidenceromnitctivitynhereely-movingat.rainesearch,4(1):171175,971.2]..cNaughton,..attaglia,.ensen,..oser,nd..oser.athntegrationndheeuralasisfhecognitiveap'.ateveurosci,(8):6638,006.3].W.atkinsnd.\", 'vector': []}, {'id_2': 'A.erkley.herientationelectivityfingleeuronsnattriateortex.xperimentalrainesearch,9:43346,974.4].en- Yishai,..ar', 'vector': []}, {'id_3': '-Or,nd.ompolinsky.heoryfrientationuningnisualortex.rocatlcadci,2(9):3844,995.5]..rown,..', 'vector': []}, {'id_4': 'rank,.ang,.. uirk,nd..', 'vector': []}, {'id_5': 'ilson.tatisticalaradigmoreuralpikerainecodingppliedoositionredictionromnsembleiringatternsfatippocampallaceells.eurosci,8(18):741125,998.6].eneve, ..atham,nd.ouget.eadingopulationodes:euralmplementationfdealbservers.ateurosci,(8):740,999.', 'vector': []}, {'id_6': '1067]..', 'vector': []}, {'id_8': 'eck,..', 'vector': []}, {'id_9': 'atham,nd.ouget.ayesiannferenceithrobabilisticopulationodes.ateurosci,(11):1432,006.8].irenbergnd..', 'vector': []}, {'id_10': 'atham.ecodingeuronalpikerains:owmportantreorrelations?roceedingsfheationalcademyfciencesfhenitedtatesfmerica,00(12):7348353,003.9].. verbeck, ..atham,nd.ouget.euralorrelations,opulationodingndomputation.ateveurosci,(5):3586,006.10].chneidman,.erryI,.egev,nd.ialek.eakairwiseorrelationsmplytronglyorrelatedetworktatesneuralopulation.ature,40(20):1007012,006.11].urtond.tskov.ellroupsevealtructureftimuluspace.', 'vector': []}, {'id_11': 'LoSomputationaliology,(10),008.12]zraillernderndturmfels.ombinatorialommutativelgebra.raduateextsnathematics.pringer,005.13]ichardtanley.ombinatoricsndommutativelgebra.rogressnathematics.irkhauseroston,004.14].arrah,.aubenbacher,.tigler,nd.tillman.everse-engineeringfolynomialynamicalystems.dvancesnppliedathematics,9:47789,007.15]laneliz-Cuba.nlgebraicpproachoeversengineeringiniteynamicalystemsrisingromiology.', 'vector': []}, {'id_12': 'IAMournalnppliedynamicalystems,1(1):318,012.', 'vector': []}, {'id_13': '10716]nnehiunderndturmfels.iphonsnhemicaleactionetworks.ulletinfathematicaliology,2(6):1448463,010.17]iovanniistone,vaiccomagno,ndenry.ynn.lgebraictatistics,olume9fonographsntatisticsndppliedrobability.hapmanall/CRC,ocaaton,L,001.omputationalommutativelgebrantatistics.18].urto,.tskov,.eliz-Cuba,ndoungs.heeuraling:nlgebraicoolornalyzinghentrinsictructurefeuralodes.ulletinfathematicaliology,5(9):1571611,013.19].chneidman,.uchalla,.egev,.arris,.ialek,nd.erryI.ynergyromilencenombinatorialeuralode.rXiv:q-bio.', 'vector': []}, {'id_14': \"NC/0607017,006.20].sborne,.almer,.isberger,nd.ialek.heeuralasisorombinatorialodingnorticalopulationesponse.ournalfeuroscience,8(50):135223531,008.21].afting,.yhn,.olden,.-B.oser,nd..oser.icrostructurefpatialapnhentorhinalortex.ature,36:80106,ug005.22]udwiganzer,rankorunbaum,ndictorlee.elly'sheoremndtselatives.nroc.ympos.ureath.,ol.II, ages0180.mer.ath.oc.,rovidence,.I.,963.23]llenatcher.lgebraicopology.ambridgeniversityress,ambridge,002.24]ilalai.haracterizationfvectorsfamiliesfonvexetsnd.ecessityfckhoff'sonditions.srael.ath.,8(2-3):17595,984.\", 'vector': []}, {'id_15': \"10825]ilalai.haracterizationfvectorsfamiliesfonvexetsndI.ufficiencyfckhoff'sonditions..\", 'vector': []}, {'id_16': \"ombin.heoryer.,1(2):16788,986.26]avidox,ohnittle,ndonal'Shea.deals,arieties,ndlgorithms.ndergraduateextsnathematics.pringer-Verlag,ework,econddition,997.nntroductionoomputationallgebraiceometryndommutativelgebra.27]..tiyahnd..acdonald.ntroductionoommutativelgebra.\", 'vector': []}]}, 'weighted_queries': {}, 'snippet_vector': []}, {'id_3': 'snippet', 'concepts': [], 'elements': {'id': 'paragraph_3', 'label': 'EXISTENCEFHISELATIONNDICATESHATUPPHILEHEREOESXISTNUCHHATHISONTRADICTSHESSUMPTIONHATUPPSIMPLICIALOMPLEX.EONCLUDEHATCASOYPEELATIONS', 'sentences': [{'id_1': 'existencefhiselationndicateshatupphilehereoesxistnuchhathisontradictshessumptionhatuppsimplicialomplex.eoncludehatCasoypeelations.', 'vector': []}, {'id_2': '40heanonicalormfChusnablessommediatelyeadff,iaheypeelations,heinimalorbiddenacesfheimplicialomplexCssociatedoheode,ndlsoheinimaleviationsfromeingimplicialomplex,hichreapturedyheypendypeelations.', 'vector': []}]}, 'weighted_queries': {}, 'snippet_vector': []}, {'id_4': 'snippet', 'concepts': [], 'elements': {'id': 'paragraph_4', 'label': \"[1].'KEEFEND.OSTROVSKY.HEIPPOCAMPUSSPATIALAP.RELIMINARYVIDENCEROMNITCTIVITYNHEREELY-MOVINGAT.RAINESEARCH,4(1):171175,971.2]..CNAUGHTON,..ATTAGLIA,.ENSEN,..OSER\", 'sentences': [{'id_1': \"[1].'Keefend.ostrovsky.heippocampusspatialap.reliminaryvidenceromnitctivitynhereely-movingat.rainesearch,4(1):171175,971.2]..cNaughton,..attaglia,.ensen,..oser,nd..oser.athntegrationndheeuralasisfhecognitiveap'.ateveurosci,(8):6638,006.3].W.atkinsnd.\", 'vector': []}, {'id_2': 'A.erkley.herientationelectivityfingleeuronsnattriateortex.xperimentalrainesearch,9:43346,974.4].en- Yishai,..ar', 'vector': []}, {'id_3': '-Or,nd.ompolinsky.heoryfrientationuningnisualortex.rocatlcadci,2(9):3844,995.5]..rown,..', 'vector': []}, {'id_4': 'rank,.ang,.. uirk,nd..', 'vector': []}, {'id_5': 'ilson.tatisticalaradigmoreuralpikerainecodingppliedoositionredictionromnsembleiringatternsfatippocampallaceells.eurosci,8(18):741125,998.6].eneve, ..atham,nd.ouget.eadingopulationodes:euralmplementationfdealbservers.ateurosci,(8):740,999.', 'vector': []}, {'id_6': '1067]..', 'vector': []}, {'id_8': 'eck,..', 'vector': []}, {'id_9': 'atham,nd.ouget.ayesiannferenceithrobabilisticopulationodes.ateurosci,(11):1432,006.8].irenbergnd..', 'vector': []}, {'id_10': 'atham.ecodingeuronalpikerains:owmportantreorrelations?roceedingsfheationalcademyfciencesfhenitedtatesfmerica,00(12):7348353,003.9].. verbeck, ..atham,nd.ouget.euralorrelations,opulationodingndomputation.ateveurosci,(5):3586,006.10].chneidman,.erryI,.egev,nd.ialek.eakairwiseorrelationsmplytronglyorrelatedetworktatesneuralopulation.ature,40(20):1007012,006.11].urtond.tskov.ellroupsevealtructureftimuluspace.', 'vector': []}, {'id_11': 'LoSomputationaliology,(10),008.12]zraillernderndturmfels.ombinatorialommutativelgebra.raduateextsnathematics.pringer,005.13]ichardtanley.ombinatoricsndommutativelgebra.rogressnathematics.irkhauseroston,004.14].arrah,.aubenbacher,.tigler,nd.tillman.everse-engineeringfolynomialynamicalystems.dvancesnppliedathematics,9:47789,007.15]laneliz-Cuba.nlgebraicpproachoeversengineeringiniteynamicalystemsrisingromiology.', 'vector': []}, {'id_12': 'IAMournalnppliedynamicalystems,1(1):318,012.', 'vector': []}, {'id_13': '10716]nnehiunderndturmfels.iphonsnhemicaleactionetworks.ulletinfathematicaliology,2(6):1448463,010.17]iovanniistone,vaiccomagno,ndenry.ynn.lgebraictatistics,olume9fonographsntatisticsndppliedrobability.hapmanall/CRC,ocaaton,L,001.omputationalommutativelgebrantatistics.18].urto,.tskov,.eliz-Cuba,ndoungs.heeuraling:nlgebraicoolornalyzinghentrinsictructurefeuralodes.ulletinfathematicaliology,5(9):1571611,013.19].chneidman,.uchalla,.egev,.arris,.ialek,nd.erryI.ynergyromilencenombinatorialeuralode.rXiv:q-bio.', 'vector': []}, {'id_14': \"NC/0607017,006.20].sborne,.almer,.isberger,nd.ialek.heeuralasisorombinatorialodingnorticalopulationesponse.ournalfeuroscience,8(50):135223531,008.21].afting,.yhn,.olden,.-B.oser,nd..oser.icrostructurefpatialapnhentorhinalortex.ature,36:80106,ug005.22]udwiganzer,rankorunbaum,ndictorlee.elly'sheoremndtselatives.nroc.ympos.ureath.,ol.II, ages0180.mer.ath.oc.,rovidence,.I.,963.23]llenatcher.lgebraicopology.ambridgeniversityress,ambridge,002.24]ilalai.haracterizationfvectorsfamiliesfonvexetsnd.ecessityfckhoff'sonditions.srael.ath.,8(2-3):17595,984.\", 'vector': []}, {'id_15': \"10825]ilalai.haracterizationfvectorsfamiliesfonvexetsndI.ufficiencyfckhoff'sonditions..\", 'vector': []}, {'id_16': \"ombin.heoryer.,1(2):16788,986.26]avidox,ohnittle,ndonal'Shea.deals,arieties,ndlgorithms.ndergraduateextsnathematics.pringer-Verlag,ework,econddition,997.nntroductionoomputationallgebraiceometryndommutativelgebra.27]..tiyahnd..acdonald.ntroductionoommutativelgebra.\", 'vector': []}]}, 'weighted_queries': {}, 'snippet_vector': []}]}\n",
      "{'id': 'VISUALHARACTERECOGNITIONSINGRTIFICIALEURALETWORKS', 'url': 'data/txt/0505016v1.pdf.txt', 'knowledge': []}\n",
      "{'id': 'PROBABILISTICEURALROGRAMS\\n\\nKENTON.URRAYDEPARTMENTFOMPUTERCIENCENDNGINEERINGNIVERSITYFOTREAMEOUTHEND,N6617MURRAY4@ND.EDU', 'url': 'data/txt/1612.00712v1.pdf.txt', 'knowledge': []}\n",
      "{'id': 'PARAMETRICALEURALETWORKSNDOMETHERIMILARRCHITECTURES\\n\\nLEONID.ITINSKII', 'url': 'data/txt/0608073v1.pdf.txt', 'knowledge': []}\n",
      "{'id': 'JOURNALFUTOMATICONTROLNDOMPUTERCIENCE', 'url': 'data/txt/0504056v1.pdf.txt', 'knowledge': []}\n",
      "{'id': 'ONEEIRDRICKORARALLELIZINGONVOLUTIONALEURALETWORKS', 'url': 'data/txt/1404.5997v2.pdf.txt', 'knowledge': []}\n",
      "{'id': 'THISAPERNVESTIGATESWOIFFERENTEURALRCHITECTURESORHEASKFELATIONLASSIFICATION', 'url': 'data/txt/1605.07333v1.pdf.txt', 'knowledge': []}\n",
      "{'id': 'UNARYODINGOREURALETWORKEARNING', 'url': 'data/txt/1009.4495v1.pdf.txt', 'knowledge': []}\n",
      "{'id': 'DEEPEURALETWORKSRIEFISTORYRZYSZTOF.IOSIRGINIAOMMONWEALTHNIVERSITYNDITISOLISHCADEMYFCIENCESNTRODUCTIONNHISRTICLEEESCRIBEEEPEURALETWORKSDNN),HEIRISTORY,NDOMEELATEDORK', 'url': 'data/txt/1701.05549v1.pdf.txt', 'knowledge': []}\n"
     ]
    }
   ],
   "source": [
    "with SqliteDict('./data/knowledge.sqlite') as db:\n",
    "    print('Library has {} items.\\n'.format(len(db)))\n",
    "    for i,j in db.items():\n",
    "        print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = open_txt('sample_doc.txt')\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsn = schema(text, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = json.dumps(jsn).lower()\n",
    "doc = re.sub('\"|\\{|\\}|\\[|\\]|\\(|\\)|:|,|\\.|^\\s+|\\d+|\\@\\w+', '', doc)\n",
    "doc = re.sub('\\s+', ' ', doc).strip()\n",
    "doc = nlp(doc)\n",
    "L = [w for w in doc if (not w.is_stop and len(w.text) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_json_nlp(jsn):\n",
    "    doc = json.dumps(jsn).lower()\n",
    "    doc = re.sub('\"|\\{|\\}|\\[|\\]|\\(|\\)|:|,|\\.|^\\s+|\\d+|\\@\\w+', '', doc)\n",
    "    doc = re.sub('\\s+', ' ', doc).strip()\n",
    "    doc = nlp(doc)\n",
    "    return [str(w) for w in doc if (not w.is_stop and len(w.text) > 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_json_fast(jsn):\n",
    "    doc = json.dumps(jsn).lower()\n",
    "    doc = re.sub('\"|\\{|\\}|\\[|\\]|\\(|\\)|:|,|\\.|^\\s+|\\d+|\\@\\w+', '', doc)\n",
    "    doc = re.sub('\\s+', ' ', doc).strip()\n",
    "    return doc.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def term_frequency(query, doc):\n",
    "    normalized_doc = normalize_json(doc)\n",
    "    return normalized_doc.count(query.lower()) / float(len(normalized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_frequency('networks', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(query, text):\n",
    "    query = ' '.join([t.lemma_ for t in nlp(query)])\n",
    "    hits = 0\n",
    "    with SqliteDict('./data/knowledge.sqlite') as db:\n",
    "        for doc in db:\n",
    "            if query in normalize_json_fast(json.dumps(db[doc])):\n",
    "                hits += 1\n",
    "        if hits > 0:\n",
    "            return (1.0 + math.log(float(len(db))) / hits) * term_frequency(query, text)\n",
    "        else:\n",
    "            return 1.0 * term_frequency(query, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf('neural', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query(term):\n",
    "    L = []\n",
    "    with SqliteDict('./data/knowledge.sqlite') as db:\n",
    "        for doc in db:\n",
    "            L.append((tf_idf(term, normalize_json_fast(json.dumps(db[doc]))),\n",
    "                      db[doc]['@id'],\n",
    "                      db[doc]['@url']))\n",
    "    return sorted(L, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query('neural')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note that while this works its pretty slow on only 9 documents. It needs to be able to search the entire database of 5000 documents. I plan on using a combination of binary search to find those documents the term exists in and add them to a list, then do the tf-idf analysis with arrays to vectorize the computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ancillus",
   "language": "python",
   "name": "ancillus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
